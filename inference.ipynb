{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7c2348c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import copy\n",
    "import os \n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from typing import Callable, Sequence, Union, List, Dict\n",
    "from torchvision import transforms\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5afd0727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_if_missing(_dir):\n",
    "    if not os.path.exists(_dir):\n",
    "        os.makedirs(_dir, exist_ok=True)\n",
    "\n",
    "def _get_instance(module, config, *args):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        module (module): The python module.\n",
    "        config (Box): The config to create the class object.\n",
    "    Returns:\n",
    "        instance (object): The class object defined in the module.\n",
    "    \"\"\"\n",
    "    cls = getattr(module, config.name)\n",
    "    kwargs = config.get('kwargs')\n",
    "    return cls(*args, **config.kwargs) if kwargs else cls(*args)\n",
    "\n",
    "def save_video(save_path, frames, fps=24, resolution=(240, 240)):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        frames: list of np arrays\n",
    "    \"\"\"\n",
    "    writer = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, resolution)\n",
    "    for frame in frames:\n",
    "        frame = frame.astype(np.uint8)\n",
    "        writer.write(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    writer.release()\n",
    "\n",
    "def read_frame_from_video(video_path, im_size=(240, 240), is_color=True):\n",
    "    \"\"\"\n",
    "    Return:\n",
    "        frames: list of np arrays\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = frame if is_color else frame[:, :, 0]\n",
    "        frame = cv2.resize(frame, im_size)\n",
    "        frames.append(frame.astype(np.uint8)[..., ::-1])\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "def frame2tensor(frame, device=None):\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "    tensor = transform(frame.copy())\n",
    "    tensor = tensor.unsqueeze(0) \n",
    "    return tensor.to(device) if device is not None else tensor\n",
    "\n",
    "def tensor2frame(tensor):\n",
    "    # invTrans = transforms.Compose([\n",
    "    #     transforms.Normalize([0., 0., 0.], [1/0.229, 1/0.224, 1/0.225]),\n",
    "    #     transforms.Normalize([-0.485, -0.456, -0.406], [ 1., 1., 1. ]),\n",
    "    # ])\n",
    "    # inv_tensor = invTrans(tensor)\n",
    "    # tensor = tensor.squeeze().permute(1, 2, 0).contiguous()\n",
    "    # frame = tensor.cpu().detach().numpy()\n",
    "    # return frame\n",
    "    tensor = tensor.squeeze().permute(1, 2, 0).contiguous()\n",
    "    frame = tensor.cpu().detach().numpy()\n",
    "    frame = frame - frame.min()\n",
    "    frame = frame / frame.max()\n",
    "    frame = frame * 255\n",
    "    return frame.astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a62ce5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_root_folder = '/home/tungi/RealTimeVideoInpainting/checkpoints/l15'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30699ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained models...\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ruamel'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m\n\u001b[1;32m      7\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124min_channels\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout_channels\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_features\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m64\u001b[39m]\n\u001b[1;32m     11\u001b[0m }\n\u001b[1;32m     12\u001b[0m net \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 14\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m net\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnet\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     16\u001b[0m net \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.conda/envs/rv/lib/python3.8/site-packages/torch/serialization.py:607\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    605\u001b[0m             opened_file\u001b[38;5;241m.\u001b[39mseek(orig_position)\n\u001b[1;32m    606\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mload(opened_file)\n\u001b[0;32m--> 607\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n",
      "File \u001b[0;32m~/.conda/envs/rv/lib/python3.8/site-packages/torch/serialization.py:882\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m    880\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m    881\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m--> 882\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    884\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m    886\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.conda/envs/rv/lib/python3.8/site-packages/torch/serialization.py:875\u001b[0m, in \u001b[0;36m_load.<locals>.UnpicklerWrapper.find_class\u001b[0;34m(self, mod_name, name)\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_class\u001b[39m(\u001b[38;5;28mself\u001b[39m, mod_name, name):\n\u001b[1;32m    874\u001b[0m     mod_name \u001b[38;5;241m=\u001b[39m load_module_mapping\u001b[38;5;241m.\u001b[39mget(mod_name, mod_name)\n\u001b[0;32m--> 875\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ruamel'"
     ]
    }
   ],
   "source": [
    "#### Collect checkpoints\n",
    "checkpoint_path = os.path.join(checkpoint_root_folder, 'stb', 'checkpoints/model_best.pth')\n",
    "#### Collect pre-trained models\n",
    "device = torch.device('cuda:0')\n",
    "print('Loading pre-trained models...')\n",
    "cls = getattr(model.net, 'STBNet')\n",
    "kwargs = {\n",
    "    'in_channels': 3,\n",
    "    'out_channels': 3,\n",
    "    'num_features': [16, 32, 64]\n",
    "}\n",
    "net = cls(**kwargs)\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "net.load_state_dict(checkpoint['net'])\n",
    "net = net.to(device)\n",
    "net = net.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6688bab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Collect input tiles\n",
    "input_path = '/home/tungi/RealTimeVideoInpainting/preprocess/videos/l15/input.mp4'\n",
    "frames = read_frame_from_video(input_path)\n",
    "input_tensors = []\n",
    "for f in frames:\n",
    "    input_tensors.append(frame2tensor(f, device=device))\n",
    "\n",
    "# Run prediction and save videos\n",
    "fps = 24\n",
    "resolution = (960, 960)\n",
    "output_dir = '/home/tungi/RealTimeVideoInpainting/preprocess/videos/l15/predicted_frames'\n",
    "mkdir_if_missing(output_dir)\n",
    "\n",
    "# Calculate latency\n",
    "timestamp = time.time()\n",
    "\n",
    "predicted_frames = [] \n",
    "# For each frame\n",
    "for tensor in input_tensors:\n",
    "    prediction = net(tensor)\n",
    "    frame = tensor2frame(prediction)\n",
    "    predicted_frames.append(frame)\n",
    "\n",
    "\n",
    "# Save the video\n",
    "save_video_path = os.path.join(output_dir, f'prediction.mp4')\n",
    "save_video(save_video_path, predicted_frames, fps=fps, resolution=resolution)\n",
    "\n",
    "total_inf_latency = time.time() - timestamp\n",
    "inf_time_per_frame = total_inf_latency / len(input_tensors)\n",
    "print(f'Average inference time per frame: total latency {total_inf_latency} / N of frames {len(input_tensors)} = {inf_time_per_frame}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt112",
   "language": "python",
   "name": "pt112"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
